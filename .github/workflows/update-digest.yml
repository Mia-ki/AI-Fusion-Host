import requests
import feedparser
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
from jinja2 import Template

# --- Config ---
ARXIV_QUERY = "artificial intelligence"
ARXIV_MAX_RESULTS = 5
TEMPLATE_PATH = "latest.html"
OUTPUT_PATH = "latest.html"

# --- Helper: Get Monday's date ---
def get_week_date():
    today = datetime.utcnow()
    monday = today - timedelta(days=today.weekday())
    return monday.strftime("%b %d, %Y")

# --- Fetch arXiv papers ---
def fetch_arxiv_papers():
    base_url = "http://export.arxiv.org/api/query"
    params = {
        "search_query": f"all:{ARXIV_QUERY}",
        "start": 0,
        "max_results": ARXIV_MAX_RESULTS,
        "sortBy": "submittedDate",
        "sortOrder": "descending"
    }
    feed = feedparser.parse(base_url + "?" + "&".join(f"{k}={v}" for k, v in params.items()))
    papers = []
    for entry in feed.entries:
        papers.append({
            "title": entry.title,
            "url": entry.link,
            "summary": BeautifulSoup(entry.summary, "html.parser").text.strip()
        })
    return papers

# --- Generate HTML blocks ---
def build_html_block(items, is_research=False):
    html = ""
    for item in items:
        if is_research:
            html += f'<div class="item"><span class="tag">ðŸ“„</span> {item[{item["title"]}</a><br/>{item["summary"]}</div>\n'
        else:
            html += f'<div class="item"><span class="tag">ðŸ”¹</span> {item[{item["title"]}</a></div>\n'
    return html

# --- Static content for testing (replace with live updates later) ---
def get_industry_updates():
    return [
        {"title": "GPT-4 Turbo Launches with 128k Context", "url": "https://openai.com/blog/gpt-4-turbo"},
        {"title": "Hugging Face Inference Endpoints for Enterprises", "url": "https://huggingface.co/blog/inference-endpoints"},
        {"title": "AI in Insurance: Claims Automation at Prudential", "url": "https://example.com/insurance-ai-claims"}
    ]

def get_metrics_block():
    return """
    <div class="item"><strong>VC Funding:</strong> AI startups raised <strong>$2.4B</strong> globally in October 2025.</div>
    <div class="item"><strong>Model Usage:</strong> Hugging Face reports a <strong>42%</strong> increase in hosted model downloads.</div>
    <div class="item"><strong>Developer Adoption:</strong> GitHub Copilot usage grew by <strong>18%</strong>.</div>
    """

def get_pm_corner():
    return """
    <div class="item"><strong>Skill Focus:</strong> Learn how to define AI product metrics â€” precision, recall, latency, and user satisfaction.</div>
    <div class="item"><strong>Tooling:</strong> Explore LangChain and LlamaIndex for building retrieval-augmented generation (RAG) systems.</div>
    <div class="item"><strong>Use Case:</strong> Prudential's AI chatbot reduced call center load by 35% â€” a case study in ROI-driven PM strategy.</div>
    """

# --- Main ---
def main():
    week_date = get_week_date()
    papers = fetch_arxiv_papers()
    industry = get_industry_updates()
    metrics_block = get_metrics_block()
    pm_corner = get_pm_corner()

    with open(TEMPLATE_PATH, "r", encoding="utf-8") as f:
        template = Template(f.read())

    output = template.render(
    WEEK_DATE=week_date,
    RESEARCH_BLOCK=build_html_block(papers, is_research=True),
    INDUSTRY_BLOCK=build_html_block(industry),
    METRICS_BLOCK=metrics_block,
    PM_BLOCK=pm_corner
    )

    with open(OUTPUT_PATH, "w", encoding="utf-8") as f:
        f.write(output)

if __name__ == "__main__":
    main()
